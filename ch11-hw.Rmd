---
title: 'MECH481A6: Engineering Data Analysis in R'
subtitle: 'Chapter 11 Homework: Modeling' 
author: 'Flynn Nyman'
date: '`r format(Sys.time(), "%d %B, %Y")`'
output: pdf_document
---

```{r global-options, include=FALSE}
# set global options for figures, code, warnings, and messages
knitr::opts_chunk$set(fig.width=6, fig.height=4, fig.path="../figs/",
                      echo=FALSE, warning=FALSE, message=FALSE)
```

# Load packages

```{r load-packages, message=FALSE}
# load packages for current session
library(tidyverse) 
library(gridExtra) 
library(dplyr)
library(ggplot2)
```

# Chapter 11 Homework

This homework will give you experience with OLS linear models and testing their assumptions.  

For this first problem set, we will examine issues of ***collinearity among predictor variables*** when fitting an OLS model with two variables. As you recall, assumption 3 from OLS regression requires there be no *collinearity* among predictor variables (the $X_i$'s) in a linear model.  The reason is that the model struggles to assign the correct $\beta_i$ values to each predictor when they are strongly correlated.   

## Question 1
Fit a series of three linear models on the `bodysize.csv` data frame using `lm()` with `height` as the dependent variable:  
  1. Model 1: use `waist` as the independent predictor variable:  
        - `formula = height ~ waist`   
  2. Model 2: use `mass` as the independent predictor variable:  
        - `formula = height ~ mass`  
  3. Model 3: use `mass + waist` as a linear combination of predictor variables:  
        - `formula = waist + mass`  
    
Report the coefficients for each of these models.  What happens to the sign and magnitude of the `mass` and `waist` coefficients when the two variables are included together?  Contrast that with the coefficients when they are used alone.

Evaluate assumption 3 about whether there is collinearity among these variables.  Do you trust the coefficients from model 3 after having seen the individual coefficients reported in models 1 and 2?

The coefficient for model 1 is 0.11, for model 2 it is 0.202, the coefficient for waist in model 3 is -0.64, and 0.64 for the mass in model 3. In model 3 the waist coefficient has a negative sign while the mass coefficient has a positive sign. When they are used alone they both have a positive value that is much lower in magnitude from when you combine them.

```{r ch11-homework-q1, echo=FALSE, include=FALSE}
data <- read.csv("./bodysize.csv")
#Model 1
model1 <- lm(formula = height ~ waist, data = data)

#Model 2
model2 <- lm(formula = height ~ mass, data = data)

#Model 3
model3 <- lm(formula = height ~ waist + mass, data = data)

```

## Question 2
Create a new variable in the `bodysize` data frame using `dplyr::mutate`. Call this variable `volume` and make it equal to $waist^2*height$.  Use this new variable to predict `mass`.  

```{r ch11-homework-q2}
data <- data %>%
  mutate(volume = (waist^2)*height)
model4 <- lm(formula = mass ~ volume, data = data)
```

Does this variable explain more of the variance in `mass` from the NHANES data? How do you know? (hint: there is both *process* and *quantitative* proof here)

Yes this model likely explains more variance for a couple reasons. For one, volume and mass have a correlation that makes the most sense compared to circumference and height. As the volume of an object increases, the mass will definitely increase. Furthermore, looking at the R-squared value of model 4 shows a value of 0.877.


Create a scatterplot of `mass` vs. `volume` to examine the fit.  Draw a fit line using `geom_smooth()`.

```{r ch11-homework-q2b}
p1 <- ggplot(data = data) +
  geom_point(aes(x = mass, y = volume),
             alpha = 0.1,
             color = "maroon4") +
  xlab("Mass (kg)") +
  ylab("Volume (cm^3)") +
  theme_classic(base_size = 13) +
  geom_smooth(data = data,
              aes(x = mass, y = volume),
              method = "lm",
              formula = "y ~ x",
              color = "black")
print(p1)

```

## Question 3
Load the `cal_aod.csv` data file and fit a linear model with `aeronet` as the independent variable and `AMOD` as the independent variable. 
```{r ch11-homework-q3}
# load data
data_2 <- read.csv("./cal_aod.csv")
model5 <- lm(formula = amod ~ aeronet, data = data_2)

```

Evaluate model assumptions 4-7 from the coursebook.  Are all these assumptions valid? 

```{r ch11-homework-q3a}
#assumption 4: mean of residuals is zero
residuals <- resid(model5)
mean_residuals <- mean(residuals)
print(mean_residuals)
```
The sum of the residuals comes out to a number with an exponent of -18 which can be treated as zero.
```{r ch11-homework-q3b}
#assumption 5: residuals are normally distributed
p2 <- ggplot(data  = model5$model, aes(sample = model5$residuals)) +
  geom_qq(alpha = 0.1,
          color = "royalblue2") +
  geom_qq_line(color = "grey") +
  ggtitle("Model 1: amod ~ aeronet") +
  theme_classic()
print(p2)

```
The residuals are normally distributed enough because they follow the quantile plots within reason. There is some stray poiunts at the upper and lower ends.
```{r ch11-homework-q3c}
#assumption 6: the error term is homoscedastic
p3 <- ggplot(data = model5$model) + 
  geom_point(aes(x = model5$fitted.values, y =model5$residuals),
             alpha = 0.25,
             color = "maroon3") +
  geom_hline(yintercept = 0) +
  theme_classic() +
  theme(aspect.ratio = 0.5)
print(p3)

```
The residuals do not show major change across the range of fitted values. This means they are homoscedastic.
```{r ch11-homework-q3d}
#assumption 7: no autocorrelation among residuals
stats::pacf(model5$residuals, 
            main = "Model 5 Partial Autocorrelation Plot")


```
This shows there is not autocorrelation between values in model 5. 